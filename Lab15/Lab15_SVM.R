renv::init() # инициализация виртуального окружения
renv::install("e1071", "ROCR") # установка библиотеки из CRAN
renv::snapshot() # делаем снимок версий библиотек в нашем виртуальном окружении
# фиксируем этот список в .lock-файле для возможности восстановления
# renv::restore() # команда отктиться к предыдушему удачному обновления библиотек

# ------------------- 
# Лабораторная работа №15:
# Машины опорных векторов (SVM).

library(e1071)
library(ROCR)

# библиотека e1071 позволяет реализовать набор методов статистического обучения
# для построения классификатора на опорных векторах можно воспользоваться svm()
# аргумент kernel = "linear" строит линейную границу между классами,
# аргумент cost  позволяет задать штраф за нарушение границ зазора.

# При малом значение аргумента cost зазор будет широким и многие опорные вектора
# будут лежать на границах зазора или выходить из них

# При большом значении аргумента cost  зазор будет узким и лишь немногие опорные 
# векторы будут лежать на его границах или выходить за них

set.seed(1)
x = matrix(rnorm(20*2), ncol = 2)
y = c(rep(-1, 10), rep(1, 10))
x[y == 1,] = x[y == 1, ] + 1

# проверим, являются ли классы линейно-разделимыми
plot(x, col = (3 - y))

# как видно из диаграммы, точки данных разных классов визуально линейно не разделимы

# построим SVM классификатор. Обратите внимание, для классификатора нам понадобится
# закодировать отклик в виде факторной переменной

dat = data.frame(x = x, y = as.factor(y))
svmfit = svm(y ~., data = dat, kernel = "linear",
             cost = 10, scale = F) # здесь мы не хотим нормализовывать данные, поэтому scale = FALSE
plot(svmfit, dat)
# опорные векторы изображены в виде крестиков, остальные наблюдения - кружки
# есть 7 опорных векторов, смотрим - какие:
svmfit$index
summary(svmfit)
# Number of Support Vectors: 7  говорит нам, что использовано 7 опорных векторов
# (4 3) из которых 4 опорных вектора принадлежат одному классу, а 3 - другому

# что будет, если уменьшить размер штрафа?
svmfit <- svm(y ~., data = dat, kernel = "linear", cost = 0.1, scale = F)
plot(svmfit, dat)
svmfit$index
summary(svmfit)

# опорных векторов стало больше, а зазор - шире

# величина штрафа - критично влияет на архитектуру модели, поэтому важно подобрать оптимальную его величину
# воспользуемся ф-ей tune() и зададим интервал для поиска значений штрафа
# по-умолчанию, используется 10-кратная перекрестная проверка
set.seed(1)
tune.out = tune(svm, y~., data = dat, kernel = "linear",
                ranges = list(cost = c(0.001, 0.1, 1, 5, 10, 100)))
summary(tune.out)

# штраф cost = 0.1 приводит к наименьшей частоте ошибок по рез-м перекрестной проверки
# ф-ия tune() сохраняет полученную оптимальную модель
bestmod = tune.out$best.model
summary(bestmod)

# Сохраним нашу модель в RDS file, она понадобится нам в Лабораторной работе №19, 20 и 21 -------------

saveRDS(bestmod, "bestmod.rds")

# Прочитаем модель из RDS file

bestmod <- readRDS("bestmod.rds")

# ------------------------------------------------------------------------------------------------------

# ф-ия predict() позволяет предсказать класс контрольных наблюдений
# сгенерируем контрольные данные
xtest = matrix(rnorm(20*2), ncol = 2)
ytest = sample(c(-1, 1), 20, replace = T)
xtest[ytest == 1,] = xtest[ytest == 1, ] + 1
testdat = data.frame(x = xtest, y = as.factor(ytest))

# теперь предскажем метки классов для этих контрольных наблюдений
ypred = predict(bestmod, testdat)
table(predict = ypred, truth = testdat$y) # верно предсказано - считаем сумму по диагонали в таблице

# ------ SVM может быть не только с линейной границей между классами,
# но и полиномиальной (тогода аргумент - kernel = "polynomial", degree задает степень) или с радиальным ядром (kernel = "radial", gamma - задает параметр ядра)

# сгенерируем данные с нелинейной границей между классами
set.seed(1)
x = matrix(rnorm(200*2), ncol = 2)
x[1:100, ] = x[1:100, ] + 2
x[101:150, ] = x[101:150,] - 2
y = c(rep(1, 150), rep(2, 50))
dat = data.frame(x = x, y = as.factor(y))
write.csv(dat, "Test_data_for_RDS_model.csv") # этот файл понадобится нам для будущих лабораторных, тк именно на наем мы обучили нашу SVM модель

plot(x, col = y) # диаграмма демонстрирует, что граница между классами действительно - нелинейная

# данные случайными образом разделим на обучающую и контрольную выборку
# при помощи ф-ии svm() построим модель с радиальным ядром и гамма = 1
train = sample(200, 100)
svmfit = svm(y ~., data = dat[train, ], kernel = "radial",
             gamma = 1, cost = 1)
plot(svmfit, dat[train, ]) # теперь решающая граница - нелинейная


saveRDS(svmfit, "bestmod.rds")

# получим больше инф-ии об этой модели
summary(svmfit)

# модель совершает заметное число ошибок на обучающих данных
# можем снизить ошибку, повысив штраф cost
# однако, это приведет к более извилистой решающей границе и переобучению
svmfit = svm(y ~., data = dat[train, ], kernel = "radial", 
             gamma = 1, cost = 1e5)
plot(svmfit, dat[train, ])

# при помощи ф-ии tune() можем подобрать оптимальный размер гамма и штрафа с радиальным ядром:
set.seed(1)
tune.out = tune(svm, y ~., data = dat[train, ], kernel = "radial", 
                ranges = list(cost = c(0.1, 1, 10, 100, 1000),
                              gamma = c(0.5, 1, 2, 3, 4)))
summary(tune.out) # таким образом, оптимальные параметры для радиального ядра - штраф = 1, гамма = 2

# Применим predict()  для пресказания контрольных данных----------------------------------
# извлекаем часть таблицы dat, используя -train в качестве вектора с соответствующими индексными номерами:
table(true = dat[-train, "y"],
      pred = predict(tune.out$best.model,
                     newdata = dat[-train, ]))
# ошибочно классифицируем = 5+6/68+21 примерно 12% контрольных наблюдений

# -----ROC - кривые для определения частоты истинно-положительных случаев и ложно-положительных

# напишем ф-ию для расчета ROC-кривой на основе веткора наблюдений pred со значениями
# классификационной ф-ии для всех наблюдений, а также вектора truth с истинными метками классов
rocplot = function(pred, truth, ...){
  predob = prediction(pred, truth)
  perf = performance(predob, "tpr", "fpr")
  plot(perf, ...)
}

# по какую сторону решающей границы находится наблюдение - показывает знак (больше или меньше 0)
# за это отвечает атрибут decision.values = TRUE
svmfit.opt = svm(y ~., data = dat[train, ], kernel = "radial",
                 gamma = 2, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.opt, dat[train, ], 
                            decision.values = T))$decision.values

# строим график ROC-кривой:
par(mfrow = c(1,2))
rocplot(fitted, dat[train, "y"], main = "Training Data")

# SVM дает довольно точные предсказания. Увеличивая гамма можем построить еще более гибкую модель
# и повысить точность предсказания на ОБУЧАЮЩИХ данных
svmfit.flex = svm(y ~., data = dat[train, ], kernel = "radial",
                 gamma = 50, cost = 1, decision.values = T)
fitted = attributes(predict(svmfit.flex, dat[train, ], 
                            decision.values = T))$decision.values
rocplot(fitted, dat[train, "y"], main = "Training Data", add = T, col = "red")

#но нам-то нужно повысить точность на ТЕСТОВЫХ данных, а не на ОБУЧАЮЩИХ!
fitted = attributes(predict(svmfit.opt, dat[-train, ], 
                            decision.values = T))$decision.values
rocplot(fitted, dat[-train, "y"], main = "Test data")
fitted = attributes(predict(svmfit.flex, dat[-train, ], 
                            decision.values = T))$decision.values
rocplot(fitted, dat[-train, "y"], add = T, col = "red")

# результаты вычисления ROC - кривых на КОНТРОЛЬНЫХ выборках, говорит, что наиболее точные
# результаты дает гамма = 2

# когда отклик предствлен фактором, состоящим более чем из 2х уровней, SVM выполнит многоклассовую классификацию 
# по сценарию "один против одного"
# когда подаваемый в ф-ию svm() вектор со значениями отклика является количественным, а не качественным - выполнится регрессия на опорных векторах
